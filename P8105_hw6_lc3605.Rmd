---
title: "Homework #6"
author: "Lynn Chen"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(modelr)
library(readxl)
library(ggplot2)


knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1: 

```{r load and clean the data}
birthweight = read_csv("./data/birthweight.csv")

## Clean the data 

birthweight = birthweight %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = factor(babysex, levels = c("1", "2")),
    babysex = fct_recode(babysex, "Female" = "2", "Male" = "1"),
    frace = factor(frace, 
                       levels = c("1", "2", "3", "4", "5", "6", "7", "8", "9")),
    frace = fct_recode(frace, 
                       "White" = "1",
                       "Black" = "2",
                       "Asian" = "3",
                       "Puerto Rican" = "4", 
                       "Other" = "8",
                       "Unknown" = "9"),
    malform = factor(malform, labels = c("Absent", "Present")),
    mrace = factor(mrace,  c(1, 2, 3, 4, 8)),
    mrace = fct_recode(mrace, 
                       "White" = "1",
                       "Black" = "2",
                       "Asian" = "3",
                       "Puerto Rican" = "4", 
                       "Other" = "8"))

## check for missing values
apply(is.na(birthweight),2,sum)
sum(!complete.cases(birthweight))
```

* This dataset contains `r nrow(birthweight)` rows and `r ncol(birthweight)` columns. 
There are no missing values in the data!! YAY!    


For the regression model, in order to choose more meaningful set of predictors for the most appropriate model, I use stepwise regression with backward selection which will starts with a  saturated model and eliminates variables each step to find a final model that best explains the birthweight data. 

```{r stepwise}
model_1 = 
  lm(bwt ~ ., data = birthweight) %>% 
  step(direction = "backward", trace = 0) %>% 
  broom::tidy() %>% 
  knitr::kable()
```

The chosen model after performing stepwise regression include the following predictors:`babysex`, `bhead`, `blength`, `delwt`, `fincome`, `gaweeks`, `mheight`, `mrace`, `parity`, `ppwt`, and `smoken`. 

```{r fit model}
fit = lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthweight) 
summary(fit)

birthweight %>% 
    add_predictions(fit) %>% 
    add_residuals(fit) %>% 
    ggplot(aes(x = pred, y = resid)) +
    geom_point(alpha = 0.2)  + 
    geom_smooth(se = F, color = "red", method = "lm", size = 1, linetype = 2) + 
    labs(title = "Residuals vs. Predicted Values", 
       x = "Predicted", 
       y = "Residuals")
```

According to the residuals against fitted values plot, the residuals are roughly symmetrical around y = 0. The residuals seems to be evenly distributed and the normal distribution of residuals assumption is satisfied for linear regression. There are a few data points with low fitted values and high residuals, majority of data points cluster around predicted values 2500 - 4000.

### Compare models

* **Model 2**: bwt ~ blength + gaweeks

* **Model 3**: bwt ~ babysex + blength + bhead + babysex * blength + babysex * bhead + blength * bhead + babysex * blength * bhead

```{r}
model_2 = lm(bwt ~ blength + gaweeks, data = birthweight) 

model_2 %>% 
  broom::tidy() %>% 
  knitr::kable()

birthweight %>% 
  add_predictions(model_2) %>% 
  add_residuals(model_2) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.2) +
  geom_smooth(se = F, color = "red", method = "lm", size = 1, linetype = 2) + 
    labs(title = "Residuals vs. Predicted Values", 
       x = "Predicted", 
       y = "Residuals")

model_3 = lm(bwt ~ babysex * blength * bhead, data = birthweight) 
model_3 %>% 
  broom::tidy() %>% 
  knitr::kable()

birthweight %>% 
  add_predictions(model_3) %>% 
  add_residuals(model_3) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.2) +
  geom_smooth(se = F, color = "red", method = "lm", size = 1, linetype = 2) + 
    labs(title = "Residuals vs. Predicted Values", 
       x = "Predicted", 
       y = "Residuals")
```

```{r, cross validation, warning = FALSE}
birthweight_cv = 
  crossv_mc(birthweight, n = 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>% 
mutate(
    fit = map(.x = train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
    model_2 = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model_3 = map(.x = train, ~lm(bwt ~ babysex * blength * bhead, data = .x))
  ) %>% 
  mutate(
    rmse_fit = map2_dbl(.x = fit, .y = test, ~rmse(model = .x, data = .y)),
    rmse_model_2 = map2_dbl(.x = model_2, .y = test, ~rmse(model = .x, data = .y)),
    rmse_model_3 = map2_dbl(.x = model_3, .y = test, ~rmse(model = .x, data = .y))
  ) 
```

### Violin plot for RMSE 

```{r plot}
birthweight_cv %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>%   
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() +
  labs(
    title = "Cross Validation of the Models",
    x = "Models",
    y = "Root Mean Squared Error (RMSE)") 
```

* Based on the violin plot, we see that the `fit` model has the lowest relative RMSE which indicates a better fit, so we conclude that it fits the data better than `model_2` and `model_3`.


## Problem 2:

```{r, warning = FALSE}
## load the data
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything()) 
```


### Bootstrapping 

```{r, warning = FALSE}
boot_strap = 
  weather_df %>% 
  bootstrap(n = 5000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm(tmax ~ tmin, data = .x)),
    tidy = map(models, broom::tidy),
    results_glance = map(models, broom::glance)) %>% 
  unnest(tidy, results_glance) %>% 
  select(strap_number, r.squared, term, estimate) %>% 
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  ) %>% 
  janitor::clean_names() %>%
  rename(
    beta_0 = intercept,
    beta_1 = tmin
  ) %>% 
  mutate(log_fct = log(beta_0 * beta_1))
```


### R Squared

```{r}
boot_strap %>%
  ggplot(aes(x = r_squared)) +
  geom_density() +
  labs(
    title = "Distribution of" ~R^2,
    x = ~R^2) +
  theme_bw()
```

* From the $R^2$ density plot, the distribution of $R^2$ seems to follow a normal distribution, with center around 0.912. Since $R^2$ value falls between 0 and 1, it indicates how much variation in `y` can be explained by `x`. Thus, we can conclude that a majority of the variation in `tmax` is explained by `tmin`.  


### Log Function

```{r}
ggplot(data = boot_strap, aes(x = log_fct)) +
  geom_density() +
  labs(
    title = "Distribution of" ~log(hat(beta)[0] %*% hat(beta)[1]),
    x = ~log(hat(beta)[0] %*% hat(beta)[1])) +
  theme_bw()
```

* The density plot of $log(\hat{\beta}_0 * \hat{\beta}_1)$ appears to be normally distributed, and centered around 2.015.

### 

```{r}
quantile(pull(results, r_squared), probs = c(.025, .975))
quantile(pull(results, logB), probs = c(.025, .975))

quantile(boot_strap$r.squared, probs = c(0.025,0.975)) %>% 
  knitr::kable(caption = "95% CI for r.suqared")
quantile(boot_strap$log_beta, probs = c(0.025,0.975)) %>% 
  knitr::kable(caption = "95% CI for log product")

```

* We are 95% confident that the true coefficient of determination falls between 0.89 and 0.93.

* We are 95% confident that the true log of the product of the intercept and slope parameters falls between 1.96 and 2.06.
Then I will identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r.squared and log(β̂ 0∗β̂ 1).  

